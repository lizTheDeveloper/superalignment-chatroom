CONSENSUS REACHED: AI Water Consumption Methodology Framework
Date: 2025-10-28
Participants: Cynthia (optimistic researcher), Sylvia (research skeptic)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## AGREED POINTS

### 1. Core Methodological Framework ✅

**CONSENSUS:** Use Li et al. (2023) peer-reviewed methodology:

```
Water = Energy × (WUE_scope1 + PUE × WUE_scope2)
```

Where:
- WUE_scope1 = On-site cooling water (L/kWh)
- WUE_scope2 = Off-site electricity generation water (L/kWh)
- PUE = Power Usage Effectiveness (datacenter overhead)

**Evidence:**
- Li et al. (2023) arXiv:2304.03271 - peer-reviewed framework
- GPT-3 empirical validation: 5.4M L total training water
- Llama-3 empirical validation: 22M L total training water
- Google (2024) production-scale attribution methodology

### 2. Sylvia's Critique Was Valid ✅

**AGREED:** "Liters per GPU-hour" is the WRONG metric for simulation because:
- ❌ Doesn't capture computational output (idle vs active GPU)
- ❌ Doesn't account for efficiency improvements (algorithmic + hardware)
- ❌ Doesn't map to simulation mechanics (AI capability score)
- ❌ Doesn't handle training vs inference distinction

**CORRECT APPROACH:** Capability-based water via energy pathway:
```
AI Capability → Energy Consumption → Water Consumption (via WUE)
```

### 3. Three Complementary Research Sources ✅

1. **Li et al. (2023):** Training water formula with empirical GPT-3 data
2. **Google arXiv 2508.15734 (2024):** Full-stack inference attribution 
3. **Lei et al. (2025):** 10,000× variation analysis across datacenter types

**CONSENSUS:** These provide complete methodological framework for defensible attribution.

### 4. 2024-2025 Water Efficiency Data ✅

**Empirical baseline (WUE scope-1, on-site cooling):**
- AWS state-of-the-art: 0.15 L/kWh
- Microsoft Azure: 0.30 L/kWh
- Industry average: 0.5-2.0 L/kWh
- Legacy air-cooled: 2-9 L/kWh

**NVIDIA "300× improvement" debunked:**
- ✅ Real claim exists
- ❌ Comparison is vs outdated air-cooling (~45 L/kWh), NOT current best practice
- ✅ Achieves ~0.15 L/kWh (matches AWS 2025 hyperscaler baseline)

**Realistic 2025 simulation parameter:** 0.25-3.14 L/kWh total (scope-1 + PUE × scope-2)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## REMAINING UNCERTAINTIES

### High Priority (Implementation Blockers):

1. **Capability score calibration** - How to map:
   - Model parameters (GPT-3 175B, GPT-4 1.7T) → simulation capability (0-100)
   - Benchmark performance → capability score
   - Timeline: When does capability 85, 90, 95 happen?

2. **Energy per capability mapping** - Conversion factor needed:
   - GPT-3: ~90K L/point (5.4M L ÷ 60 points assumed)
   - Llama-3: ~314K L/point (22M L ÷ 70 points assumed)
   - **Requires validation:** Are capability scores linear? Logarithmic?

3. **Training vs inference ratio** - Water allocation:
   - Current: Training dominates (one-time 22M L for Llama-3)
   - 2030: Inference may dominate if billions of queries/day
   - Need trajectory model

### Medium Priority (Refinements):

4. **NVIDIA 300× specification breakdown:**
   - Per-FLOP energy improvement component?
   - Per-kW water improvement (WUE_site) component?
   - Combined vs independent effects?

5. **Grid decarbonization coupling:**
   - Solar/wind: ~0 L/kWh (WUE_scope2 ≈ 0)
   - Natural gas: ~1.0 L/kWh
   - Coal: ~2.0 L/kWh
   - Does clean energy transition reduce water proportionally?

6. **Geographic optimization modeling:**
   - 5-10× variation by location (Arizona vs Ireland)
   - Should simulation model datacenter location strategy?

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## IMPLEMENTATION RECOMMENDATIONS

### Recommended Simulation Approach:

```typescript
// Step 1: Map AI capability to energy consumption
const energyPerCapabilityPoint_kWh = deriveFromTrainingRuns();
// Anchors: GPT-3 (5.4M L / 60 points), Llama-3 (22M L / 70 points)

// Step 2: Convert energy to water using Li et al. formula
const WUE_scope1 = 0.25; // L/kWh on-site cooling (2025 baseline)
const WUE_scope2 = 3.14; // L/kWh electricity generation (US average)
const PUE = 1.15; // Datacenter overhead factor

const waterPerCapability_L = energyPerCapabilityPoint_kWh * 
  (WUE_scope1 + PUE * WUE_scope2);

// Step 3: Apply efficiency trajectory
const annualEfficiencyGain = 0.25; // -25% per year (conservative)
const year = state.currentYear;
const efficiencyMultiplier = Math.pow(0.75, year - 2025);
const thermodynamicFloor = 0.01; // L/kWh minimum

const finalWaterConsumption = Math.max(
  waterPerCapability_L * efficiencyMultiplier,
  thermodynamicFloor * energyPerCapabilityPoint_kWh
);
```

### Required Documentation:

1. ✅ Cite Li et al. (2023) for methodology
2. ✅ Document empirical anchors (GPT-3, Llama-3)
3. ✅ Specify uncertainty ranges (0.15-9 L/kWh depending on location/tech)
4. ✅ Note capability mapping requires validation
5. ✅ Include sensitivity analysis for efficiency trajectory

### Sensitivity Analysis Parameters:

- **WUE_scope1 range:** 0.15-2.1 L/kWh (best hyperscaler → legacy)
- **WUE_scope2 range:** 2.0-4.35 L/kWh (clean grid → coal-heavy)
- **Efficiency improvement:** -15% to -40% per year (conservative → optimistic)
- **Capability mapping:** Linear vs logarithmic scaling

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## KEY LEARNINGS

### What Cynthia Learned:

**"Methodological frameworks beat point estimates."**

When Sylvia says "we're solving the wrong problem," step back and find research on HOW to measure, not just WHAT to measure.

Water consumption methodology requires THREE components:
1. Empirical anchors (training run totals)
2. Allocation methodology (facility → workload)
3. Efficiency trajectory (hardware + algorithmic gains)

The "How Hungry is AI?" per-query formula provides template for defensible attribution. Lei et al.'s 10,000× variation finding shows why context matters: server efficiency, grid factors, cooling type, climate zone all compound multiplicatively.

### What Sylvia Validated:

**"Infrastructure metrics don't map directly to simulation mechanics."**

L/GPU-hour is an infrastructure metric, not a capability metric. The simulation tracks AI capability advancement, not hardware deployment. Need conversion pathway:

Hardware → Energy → Water (via WUE) ✅ VALID
Hardware → Water (direct) ❌ WRONG

Cynthia's research found the correct pathway through Li et al. (2023) and Google (2024) methodologies.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

## SUMMARY

**Question:** How should simulation model AI water consumption in 2024-2025?

**Answer:** Use Li et al. (2023) energy-to-water formula with capability-based energy mapping:

1. **Methodology:** Peer-reviewed (arXiv 2304.03271)
2. **Empirical validation:** GPT-3 (5.4M L), Llama-3 (22M L)
3. **2025 baseline:** 0.25-3.14 L/kWh total WUE
4. **Efficiency trajectory:** -25%/year improvement
5. **Uncertainties:** Capability mapping, training/inference ratio, geographic variation
6. **Required:** Sensitivity analysis + documentation of assumptions

**This is methodologically defensible** provided:
- ✅ Uncertainty ranges included
- ✅ Capability score mapping validated
- ✅ Assumptions explicitly documented
- ✅ Sensitivity analysis performed

**Next steps:**
1. Validate capability score mapping (GPT-3 → score 60?, GPT-4 → score 80?)
2. Implement Li et al. formula in simulation
3. Add uncertainty sampling for Monte Carlo runs
4. Document assumptions in wiki + code comments

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

DEBATE CONCLUDED: 2025-10-28 21:58 UTC
Both researchers agree: Framework is solid, implementation requires documented uncertainties.
